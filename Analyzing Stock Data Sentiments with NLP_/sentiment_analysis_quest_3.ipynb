{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a453cc11",
   "metadata": {},
   "source": [
    "# AI/ML with Python: Web Scraping & Sentiment Analysis\n",
    "## Analyzing Stock Data Sentiments with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7ff1b-0406-4d5d-94cc-d5e27448b731",
   "metadata": {},
   "source": [
    "In addition to leveraging social media insights and chart analysis, news analysis is a pivotal aspect of informed decision-making in the investment world. Major events, such as Twitter's high-profile takeover, have the potential to significantly impact markets. Given the relentless influx of news on a daily basis, it's impractical to manually analyze and interpret the sentiments embedded in all these news stories.\n",
    "\n",
    "This is where sentiment analysis becomes invaluable. By employing advanced sentiment analysis techniques, investors can efficiently process and understand the emotions and opinions expressed in hundreds of news headlines every day.\n",
    "\n",
    "In this exercise, we'll start by importing the required libraries and methods. Our first step involves data collection, where we'll focus on extracting headlines from all five provided HTML files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5aaafe-a9b1-4c79-b16c-972381e76e92",
   "metadata": {},
   "source": [
    "To begin, we'll begin by importing BeautifulSoup. Load the 5 html files and put them all into a list, where we will then use BeautifulSoup to scrape the information from there. **Note that the code that are marked by the comments \"TODO\" are for you to fill them up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3b607-9855-4441-806f-b23a74cf8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "html_tables = {}\n",
    "fb_path = 'fb_05ene.html'\n",
    "fb1_path = 'fb_22sep.html'\n",
    "tsla_path = 'tsla_22sep.html'\n",
    "tsla2_path = 'tsla_05ene.html'\n",
    "tsla3_path = 'tsla_26nov.html'\n",
    "\n",
    "ls = []\n",
    "ls.append(fb_path)\n",
    "ls.append(fb1_path)\n",
    "ls.append(tsla_path)\n",
    "ls.append(tsla2_path)\n",
    "ls.append(tsla3_path)\n",
    "\n",
    "# For every table in the datasets folder...\n",
    "for table_name in ls:\n",
    "    # TODO: Input the directory of your downloaded html files, for instance: f'/Users/jacky/Downloads/{table_name}'\n",
    "    table_path = \n",
    "    # Open as a python file in read-only mode\n",
    "    table_file = open(table_path, 'r')\n",
    "\n",
    "    html = BeautifulSoup(table_file)\n",
    "    # TODO: Find 'news-table' in the Soup and load it into 'html_table' (hint: use .find())\n",
    "    html_table = \n",
    "    # Add the table to our dictionary\n",
    "    html_tables[table_name] = html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b613f-a158-46ef-b84d-4d0ef264673b",
   "metadata": {},
   "source": [
    "Once we have loaded them all into the html tables, we'll attempt to print the headlines that we have extracted using BeautifulSoup. In this case, we will attempt to print out the first 4 headlines of Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6a152-b9e3-4080-b0ff-a97b3fc17343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read one single day of headlines \n",
    "fb = html_tables['fb_22sep.html']\n",
    "\n",
    "# Get all the table rows tagged in HTML with <tr> into 'fb_tr'\n",
    "fb_tr = fb.findAll('tr')\n",
    "\n",
    "# For each row...\n",
    "for i, table_row in enumerate(fb_tr):\n",
    "    \n",
    "    # TODO: Store the text of the element 'a' into 'link_text'\n",
    "    link_text = \n",
    "    \n",
    "    # TODO: Store the text of the element 'td' into 'data_text'\n",
    "    data_text = \n",
    "    \n",
    "    # Print the count\n",
    "    print(f'File number {i+1}:')\n",
    "    \n",
    "    # Print the contents of 'link_text' and 'data_text' \n",
    "    print(link_text)\n",
    "    print(data_text)\n",
    "    # The following exits the loop after four rows to prevent spamming the notebook, do not touch\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668afca-04f4-49aa-8557-c97b4740bb5a",
   "metadata": {},
   "source": [
    "If executed correctly, you should see the expected output below:\n",
    "\n",
    "File number 1:\n",
    "How former Facebook and Google engineer Bret Taylor earned Marc Benioff's trust at Salesforce\n",
    "Sep-22-18 11:08AM  \n",
    "\n",
    "File number 2:\n",
    "White House Drafts Order To Look Into Google, Facebook Practices\n",
    "12:02AM  \n",
    "\n",
    "File number 3:\n",
    "Facebook Withdraws Direct Promotion of Political Campaigns\n",
    "Sep-21-18 06:21PM  \n",
    "\n",
    "File number 4:\n",
    "Facebook's Plan to Pull Back Campaign Support to Trump in 2020\n",
    "06:18PM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f3567-374a-49b3-b67d-b9b2b76dac20",
   "metadata": {},
   "source": [
    "Next, we will attempt to pass all of the text into a list called `parsed_news`. This will contain the ticker, date, time and headline. We will be collating our extracted data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5daba-1f53-4add-a9d3-ef61d436a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold the parsed news into a list\n",
    "parsed_news = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in html_tables.items():\n",
    "    \n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "\n",
    "        # TODO: Store the text from the tr tag into text\n",
    "        text = \n",
    "\n",
    "        # Split the text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        \n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split(\"_\")[0]\n",
    "        \n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, x.a.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a64e6-0023-41ff-9830-6a803fc1e06d",
   "metadata": {},
   "source": [
    "With the data collected, we will now proceed to download ntlk. Ensure that you have 'vader_lexicon' downloaded so that we can begin using VADER for our sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8f466-86dd-4c29-8fd5-d42a3cb134c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Instantiate the sentiment intensity analyzer\n",
    "vader = \n",
    "\n",
    "# Update the lexicon\n",
    "vader.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55234fdb-a56f-4228-93d7-9d5f31ee42be",
   "metadata": {},
   "source": [
    "Having initalised VADER, we will now go back to the previous read data and turn it into a structured format. Ensure you have the pandas package installed and that you are aware of how to utilise it to build a column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e7631-77ca-4c5b-9522-1948522954fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use these column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "\n",
    "# Convert the list of lists into a DataFrame\n",
    "scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "# TODO: Iterate through the headlines and get the polarity scores\n",
    "scores = \n",
    "\n",
    "# Convert the list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames\n",
    "scored_news = scored_news.join(scores_df)\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date\n",
    "\n",
    "scored_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d8f4d-a669-4a34-8a91-1b592ee4983d",
   "metadata": {},
   "source": [
    "By the end of it, you should have 500 rows x 8 columns of fb and tsla data in a table format. \n",
    "\n",
    "Moving on, ensure you have matplotlib installed. We will do some basic analysis to find the mean compound score of these stock sentiments. If you recall, compound gives an overall sentiment analysis based on the level of neutrality, positivity and negativity of both stocks. \n",
    "\n",
    "If you have done everything correctly until now, you should able to plot a graph of the sentiment analysis of fb and tsla stocks from September of 2018 to January of 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30529822-17f2-47de-81a3-753435cc0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_c = scored_news.groupby(['date', 'ticker'])\n",
    "mean_c = mean_c['compound'].mean()\n",
    "\n",
    "# Unstack the column ticker\n",
    "mean_c = mean_c.unstack('ticker');\n",
    "\n",
    "# Plot a bar chart with pandas\n",
    "mean_c.plot.bar(figsize = (10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebf189-68d8-4181-a46a-7e09bd008351",
   "metadata": {},
   "source": [
    "Awesome, with that done, we now proceed to dig in deeper, what is the sentiment within a particular day? Let's zoom in on **3rd January 2019** (i.e. \"2019-01-03\") to analyze facebook's stock. Refer to the below code for the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9836a21-50cb-4cd8-9ea8-18ab8f209cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_news['date'] = pd.to_datetime(scored_news['date'])\n",
    "\n",
    "# TODO: Filter the dataset for the date\n",
    "filtered_df = \n",
    "filtered_df = filtered_df.set_index(['ticker', 'date'])\n",
    "filtered_df = filtered_df.xs('fb')\n",
    "\n",
    "# TODO: Set the index of filtered_df to time \n",
    "filtered_df = \n",
    "\n",
    "# TODO: Sort by index\n",
    "filtered_df = \n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19b5c0-16cb-4020-a3a9-9c17fdc1ace7",
   "metadata": {},
   "source": [
    "Once done, we will once again plot our graph to analyse the negative, neutral and positive sentiments throughout the day at different timings. Run the below code, and if you have completed the above steps correct, this should give you a colorful barcharts representing all the different sentiments throughout the day at different timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02810d-d3f0-4f74-8bd6-6e8d1b37c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = \"Negative, neutral, and positive sentiment for FB on 2019-01-03\"\n",
    "COLORS = [\"red\",\"orange\", \"green\"]\n",
    "\n",
    "# Drop the columns that aren't useful for the plot\n",
    "plot_day = filtered_df.drop(['compound', 'headline'], axis=1)\n",
    "\n",
    "# Change the column names to 'negative', 'positive', and 'neutral'\n",
    "plot_day.columns = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "plot_day.plot.bar(stacked=True, figsize=(10, 6), title=TITLE, color=COLORS).legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.ylabel('scores');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
